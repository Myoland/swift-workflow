//
//  LLMNode+OpenAI+Response.swift
//  swift-workflow
//
//  Created by AFuture on 2025/5/6.
//

import Foundation

public struct OpenAIModelReponseIncompleteDetails {
    /// The reason why the response is incomplete.
    public let reason: String
}

public struct OpenAIModelReponseError {
    /// The error code for the response.
    let code: String
    
    /// A human-readable description of the error.
    let message: String
}

public enum OpenAIModelReponseStatus: String, Codable {
    case completed
    case failed
    case in_progress
    case incomplete
}

public struct OpenAIModelReponseUsageInputTokenDetail {
    /// The number of tokens that were retrieved from the cache.
    /// More on (prompt caching)[https://platform.openai.com/docs/guides/prompt-caching].
    public let cached_tokens: Int
}

public struct OpenAIModelReponseUsageOutputTokenDetail {
    /// The number of reasoning tokens.
    public let reasoning_tokens: Int
}

public struct OpenAIModelReponseUsage {
    /// The number of input tokens.
    public let input_tokens: Int
    /// A detailed breakdown of the input tokens.
    public let input_tokens_details: OpenAIModelReponseUsageInputTokenDetail
    /// The number of output tokens.
    public let output_tokens: Int
    /// A detailed breakdown of the output tokens.
    public let output_tokens_details: OpenAIModelReponseUsageOutputTokenDetail
    /// The total number of tokens used.
    public let total_tokens: Int
}

public struct OpenAIModelReponse {
    /// Unix timestamp (in seconds) of when this Response was created.
    public let created_at: Date
    
    /// An error object returned when the model fails to generate a Response.
    public let error: OpenAIModelReponseError?
    
    /// Unique identifier for this Response.
    public let id: String
    
    /// Details about why the response is incomplete.
    public let incomplete_details: OpenAIModelReponseIncompleteDetails?
    
    /// Inserts a system (or developer) message as the first item in the model's context.
    /// When using along with `previous_response_id`, the instructions from a previous
    /// response will not be carried over to the next response.
    /// This makes it simple to swap out system (or developer) messages in new responses.
    public let instructions: String?
    
    /// An upper bound for the number of tokens that can be generated for a response,
    /// including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning?api-mode=responses).
    public let max_output_tokens: Int?
    
    /// Set of 16 key-value pairs that can be attached to an object.
    /// This can be useful for storing additional information about the
    /// object in a structured format, and querying for objects via API or the dashboard.
    ///
    /// Keys are strings with a maximum length of 64 characters.
    /// Values are strings with a maximum length of 512 characters.
    public let metadata: [String: String]
    
    /// Model ID used to generate the response, like gpt-4o or o3.
    /// OpenAI offers a wide range of models with different capabilities,
    /// performance characteristics, and price points.
    /// Refer to the [model guide](https://platform.openai.com/docs/models) to browse and compare available models.
    public let model: String
    
    /// The object type of this resource - always set to `response`.
    public let object: String
    
    /// An array of content items generated by the model.
    ///   - The length and order of items in the output array is dependent on the model's response.
    ///   - Rather than accessing the first item in the output array and assuming it's an assistant
    ///     message with the content generated by the model, you might consider using the
    ///     `output_text` property where supported in SDKs.
    public let output: [OpenAIModelReponseContext]
    
    /// SDK-only convenience property that contains the aggregated
    /// text output from all `output_text` items in the output array, if any are present.
    /// Supported in the Python and JavaScript SDKs.
    public let output_text: String?
    
    /// Whether to allow the model to run tool calls in parallel.
    /// Defaults to true
    public let parallel_tool_calls: Bool

    /// The unique ID of the previous response to the model.
    /// Use this to create multi-turn conversations.
    /// Learn more about [conversation state](https://platform.openai.com/docs/guides/conversation-state).
    public let previous_response_id: String?
    
    /// Configuration options for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
    ///
    /// o-series models only
    public let reasoning: OpenAIModelReponseRequestResoning?
    
    /// The status of the response generation.
    public let status: OpenAIModelReponseStatus
    
    /// What sampling temperature to use, between 0 and 2.
    /// Higher values like 0.8 will make the output more random,
    /// while lower values like 0.2 will make it more focused and deterministic.
    /// We generally recommend altering this or `top_p` but not both.
    public let temperature: Double?
    
    /// Configuration options for a text response from the model.
    /// Can be plain text or structured JSON data.
    ///
    /// Learn more:
    /// [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
    /// [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
    let text: openAIModelReponseRequestTextConfiguration?
    
    /// How the model should select which tool (or tools) to use when generating a response.
    /// See the tools parameter to see how to specify which tools the model can call.
    let toolChoice: OpenAIModelReponseRequestToolChoice?
    
    /// An array of tools the model may call while generating a response.
    /// You can specify which tool to use by setting the `tool_choice` parameter.
    ///
    /// The two categories of tools you can provide the model are:
    ///   - Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities,
    ///     like [web search](https://platform.openai.com/docs/guides/tools-web-search) or [file search](https://platform.openai.com/docs/guides/tools-file-search).
    ///     Learn more about [built-in](https://platform.openai.com/docs/guides/tools) tools.
    ///   - Function calls (custom tools): Functions that are defined by you,
    ///     enabling the model to call your own code.
    ///     Learn more about [function calling](https://platform.openai.com/docs/guides/function-calling).
    let tools: OpenAIModelReponseRequestTool?
    
    /// An alternative to sampling with temperature, called nucleus sampling,
    /// where the model considers the results of the tokens with `top_p` probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or `temperature` but not both.
    let topP: Double?
    
    /// The truncation strategy to use for the model response.
    /// Defaults to disabled
    let truncation: OpenAIModelReponseRequestTruncation?
    
    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
    /// [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
    let user: String?
    
    /// Represents token usage details including input tokens,
    /// output tokens, a breakdown of output tokens, and the total tokens used.
    let usage: OpenAIModelReponseUsage
}
