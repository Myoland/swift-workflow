//
//  LLMNode+OpenAI+Response.swift
//  swift-workflow
//
//  Created by AFuture on 2025/5/6.
//

import Foundation

public struct OpenAIModelReponseIncompleteDetails: Codable {
    /// The reason why the response is incomplete.
    public let reason: String
}

public struct OpenAIModelReponseError: Codable {
    /// The error code for the response.
    let code: String

    /// A human-readable description of the error.
    let message: String
}

public enum OpenAIModelReponseStatus: String, Codable {
    case completed
    case failed
    case in_progress
    case incomplete
}

public struct OpenAIModelReponseUsageInputTokenDetail: Codable {
    /// The number of tokens that were retrieved from the cache.
    /// More on (prompt caching)[https://platform.openai.com/docs/guides/prompt-caching].
    public let cached_tokens: Int
}

public struct OpenAIModelReponseUsageOutputTokenDetail: Codable {
    /// The number of reasoning tokens.
    public let reasoning_tokens: Int
}

public struct OpenAIModelReponseUsage: Codable {
    /// The number of input tokens.
    public let input_tokens: Int
    /// A detailed breakdown of the input tokens.
    public let input_tokens_details: OpenAIModelReponseUsageInputTokenDetail
    /// The number of output tokens.
    public let output_tokens: Int
    /// A detailed breakdown of the output tokens.
    public let output_tokens_details: OpenAIModelReponseUsageOutputTokenDetail
    /// The total number of tokens used.
    public let total_tokens: Int
}

public struct OpenAIModelReponse: Codable {
    /// Unix timestamp (in seconds) of when this Response was created.
    public let created_at: Int

    /// An error object returned when the model fails to generate a Response.
    public let error: OpenAIModelReponseError?

    /// Unique identifier for this Response.
    public let id: String

    /// Details about why the response is incomplete.
    public let incomplete_details: OpenAIModelReponseIncompleteDetails?

    /// Inserts a system (or developer) message as the first item in the model's context.
    /// When using along with `previous_response_id`, the instructions from a previous
    /// response will not be carried over to the next response.
    /// This makes it simple to swap out system (or developer) messages in new responses.
    public let instructions: String?

    /// An upper bound for the number of tokens that can be generated for a response,
    /// including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning?api-mode=responses).
    public let max_output_tokens: Int?

    /// Set of 16 key-value pairs that can be attached to an object.
    /// This can be useful for storing additional information about the
    /// object in a structured format, and querying for objects via API or the dashboard.
    ///
    /// Keys are strings with a maximum length of 64 characters.
    /// Values are strings with a maximum length of 512 characters.
    public let metadata: [String: String]

    /// Model ID used to generate the response, like gpt-4o or o3.
    /// OpenAI offers a wide range of models with different capabilities,
    /// performance characteristics, and price points.
    /// Refer to the [model guide](https://platform.openai.com/docs/models) to browse and compare available models.
    public let model: String

    /// The object type of this resource - always set to `response`.
    public let object: String

    /// An array of content items generated by the model.
    ///   - The length and order of items in the output array is dependent on the model's response.
    ///   - Rather than accessing the first item in the output array and assuming it's an assistant
    ///     message with the content generated by the model, you might consider using the
    ///     `output_text` property where supported in SDKs.
    public let output: [OpenAIModelReponseContext]

    /// SDK-only convenience property that contains the aggregated
    /// text output from all `output_text` items in the output array, if any are present.
    /// Supported in the Python and JavaScript SDKs.
    public let output_text: String?

    /// Whether to allow the model to run tool calls in parallel.
    /// Defaults to true
    public let parallel_tool_calls: Bool

    /// The unique ID of the previous response to the model.
    /// Use this to create multi-turn conversations.
    /// Learn more about [conversation state](https://platform.openai.com/docs/guides/conversation-state).
    public let previous_response_id: String?

    /// Configuration options for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
    ///
    /// o-series models only
    public let reasoning: OpenAIModelReponseRequestResoning?

    /// The status of the response generation.
    public let status: OpenAIModelReponseStatus

    /// What sampling temperature to use, between 0 and 2.
    /// Higher values like 0.8 will make the output more random,
    /// while lower values like 0.2 will make it more focused and deterministic.
    /// We generally recommend altering this or `top_p` but not both.
    public let temperature: Double?

    /// Configuration options for a text response from the model.
    /// Can be plain text or structured JSON data.
    ///
    /// Learn more:
    /// [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
    /// [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
    let text: openAIModelReponseRequestTextConfiguration?

    /// How the model should select which tool (or tools) to use when generating a response.
    /// See the tools parameter to see how to specify which tools the model can call.
    let toolChoice: OpenAIModelReponseRequestToolChoice?

    /// An array of tools the model may call while generating a response.
    /// You can specify which tool to use by setting the `tool_choice` parameter.
    ///
    /// The two categories of tools you can provide the model are:
    ///   - Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities,
    ///     like [web search](https://platform.openai.com/docs/guides/tools-web-search) or [file search](https://platform.openai.com/docs/guides/tools-file-search).
    ///     Learn more about [built-in](https://platform.openai.com/docs/guides/tools) tools.
    ///   - Function calls (custom tools): Functions that are defined by you,
    ///     enabling the model to call your own code.
    ///     Learn more about [function calling](https://platform.openai.com/docs/guides/function-calling).
    let tools: [OpenAIModelReponseRequestTool]?

    /// An alternative to sampling with temperature, called nucleus sampling,
    /// where the model considers the results of the tokens with `top_p` probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or `temperature` but not both.
    let topP: Double?

    /// The truncation strategy to use for the model response.
    /// Defaults to disabled
    let truncation: OpenAIModelReponseRequestTruncation?

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
    /// [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
    let user: String?

    /// Represents token usage details including input tokens,
    /// output tokens, a breakdown of output tokens, and the total tokens used.
    let usage: OpenAIModelReponseUsage?
}



struct OpenAIModelStreamResponseCreated: Codable {
    let type: OpenAIModelStreamResponseType = .response_created
    let response: OpenAIModelReponse

    enum CodingKeys: String, CodingKey {
        case type
        case response
    }
}

struct OpenAIModelStreamResponseInProgess: Codable {
    let type: OpenAIModelStreamResponseType = .response_in_progress
    let response: OpenAIModelReponse

    enum CodingKeys: String, CodingKey {
        case type
        case response
    }
}


struct OpenAIModelStreamResponseCompleted: Codable {
    let type: OpenAIModelStreamResponseType = .response_completed
    let response: OpenAIModelReponse

    enum CodingKeys: String, CodingKey {
        case type
        case response
    }
}

struct OpenAIModelStreamResponseFailed: Codable {
    let type: OpenAIModelStreamResponseType = .response_failed
    let response: OpenAIModelReponse

    enum CodingKeys: String, CodingKey {
        case type
        case response
    }
}

struct OpenAIModelStreamResponseInCompleted: Codable {
    let type: OpenAIModelStreamResponseType = .response_incomplete  
    let response: OpenAIModelReponse

    enum CodingKeys: String, CodingKey {
        case type
        case response
    }
}

struct OpenAIModelStreamResponseOutputItemAdd: Codable {
    let type: OpenAIModelStreamResponseType = .response_output_item_added
    let output_index: Int
    let item: OpenAIModelReponseContext

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item
    }
}

struct OpenAIModelStreamResponseOutputItemDone: Codable {
    let type: OpenAIModelStreamResponseType = .response_output_item_done
    let output_index: Int
    let item: OpenAIModelReponseContext

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item
    }
}

struct OpenAIModelStreamResponseContentPartAdded: Codable {
    let type: OpenAIModelStreamResponseType = .response_content_part_added
    let content_index: Int
    let item_id: String
    let output_index: Int
    let part: OpenAIModelReponseContextOutputContent

    enum CodingKeys: String, CodingKey {
        case type
        case content_index
        case item_id
        case output_index
        case part
    }
}

struct OpenAIModelStreamResponseContentPartDone: Codable {
    let type: OpenAIModelStreamResponseType = .response_content_part_done
    let content_index: Int
    let item_id: String
    let output_index: Int
    let part: OpenAIModelReponseContextOutputContent

    enum CodingKeys: String, CodingKey {
        case type
        case content_index
        case item_id
        case output_index
        case part
    }
}

struct OpenAIModelStreamResponseOutputTextDelta: Codable {
    let type: OpenAIModelStreamResponseType = .response_output_text_delta
    let content_index: Int
    let item_id: String
    let output_index: Int
    let delta: String

    enum CodingKeys: String, CodingKey {
        case type
        case content_index
        case item_id
        case output_index
        case delta
    }
}

struct OpenAIModelStreamResponseOutputTextAnnotationAdded: Codable {
    let type: OpenAIModelStreamResponseType = .response_output_text_annotation_added
    let annotation_index: Int
    let content_index: Int
    let item_id: String
    let output_index: Int
    let annotation: OpenAIModelReponseContextOutputContentTextOutputAnnotation

    enum CodingKeys: String, CodingKey {
        case type
        case annotation_index
        case content_index
        case item_id
        case output_index
        case annotation
    }
}   

struct OpenAIModelStreamResponseOutputTextDone: Codable {
    let type: OpenAIModelStreamResponseType = .response_output_text_done
    let content_index: Int
    let item_id: String
    let output_index: Int
    let text: String

    enum CodingKeys: String, CodingKey {
        case type
        case content_index
        case item_id
        case output_index
        case text
    }
}   

struct OpenAIModelStreamResponseRefusalDelta: Codable {
    let type: OpenAIModelStreamResponseType = .response_refusal_delta
    let content_index: Int
    let item_id: String
    let output_index: Int
    let delta: String

    enum CodingKeys: String, CodingKey {
        case type
        case content_index
        case item_id
        case output_index
        case delta
    }
}

struct OpenAIModelStreamResponseRefusalDone: Codable {
    let type: OpenAIModelStreamResponseType = .response_refusal_done
    let content_index: Int
    let item_id: String
    let output_index: Int
    let refusal: String

    enum CodingKeys: String, CodingKey {
        case type
        case content_index
        case item_id
        case output_index
        case refusal
    }
}   

struct OpenAIModelStreamResponseFunctionCallArgumentsDelta: Codable {
    let type: OpenAIModelStreamResponseType = .response_function_call_arguments_delta
    let content_index: Int
    let item_id: String
    let output_index: Int
    let delta: String

    enum CodingKeys: String, CodingKey {
        case type
        case content_index
        case item_id
        case output_index
        case delta
    }
}

struct OpenAIModelStreamResponseFunctionCallArgumentsDone: Codable {
    let type: OpenAIModelStreamResponseType = .response_function_call_arguments_done
    let item_id: String
    let output_index: Int
    let arguments: String

    enum CodingKeys: String, CodingKey {
        case type
        case item_id
        case output_index
        case arguments
    }
}   

struct OpenAIModelStreamResponseFileSearchCallInProgress: Codable {
    let type: OpenAIModelStreamResponseType = .response_file_search_call_in_progress
    let output_index: Int
    let item_id: String

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
    }
}   

struct OpenAIModelStreamResponseFileSearchCallSearching: Codable {
    let type: OpenAIModelStreamResponseType = .response_file_search_call_searching
    let output_index: Int
    let item_id: String

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
    }
}   

struct OpenAIModelStreamResponseFileSearchCallCompleted: Codable {
    let type: OpenAIModelStreamResponseType = .response_file_search_call_completed
    let output_index: Int
    let item_id: String

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
    }
}   

struct OpenAIModelStreamResponseWebSearchCallInProgress: Codable {
    let type: OpenAIModelStreamResponseType = .response_web_search_call_in_progress
    let output_index: Int
    let item_id: String

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
    }
}   

struct OpenAIModelStreamResponseWebSearchCallSearching: Codable {
    let type: OpenAIModelStreamResponseType = .response_web_search_call_searching
    let output_index: Int
    let item_id: String

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
    }
}      

struct OpenAIModelStreamResponseWebSearchCallCompleted: Codable {
    let type: OpenAIModelStreamResponseType = .response_web_search_call_completed
    let output_index: Int
    let item_id: String

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
    }
}          

struct OpenAIModelStreamResponseReasoningSummaryPartAdded: Codable {
    let type: OpenAIModelStreamResponseType = .response_reasoning_summary_part_added
    let output_index: Int
    let item_id: String
    let summary_index: Int
    let part: OpenAIModelReponseContextReasoningSummaryTextContent

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
        case summary_index
        case part
    }
}   

struct OpenAIModelStreamResponseReasoningSummaryPartDone: Codable {
    let type: OpenAIModelStreamResponseType = .response_reasoning_summary_part_done
    let output_index: Int
    let item_id: String
    let summary_index: Int
    let part: OpenAIModelReponseContextReasoningSummaryTextContent

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
        case summary_index
        case part
    }
}      

struct OpenAIModelStreamResponseReasoningSummaryTextDelta: Codable {
    let type: OpenAIModelStreamResponseType = .response_reasoning_summary_text_delta
    let output_index: Int
    let item_id: String
    let summary_index: Int
    let delta: String
    
    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
        case summary_index
        case delta
    }
}

struct OpenAIModelStreamResponseReasoningSummaryTextDone: Codable {
    let type: OpenAIModelStreamResponseType = .response_reasoning_summary_text_done
    let output_index: Int
    let item_id: String
    let summary_index: Int
    let text: String

    enum CodingKeys: String, CodingKey {
        case type
        case output_index
        case item_id
        case summary_index
        case text
    }
}   

struct OpenAIModelStreamResponseError: Codable {
    let type: OpenAIModelStreamResponseType = .error
    let code: String?
    let message: String
    let param: String?

    enum CodingKeys: String, CodingKey {
        case type
        case code
        case message
        case param
    }
}

enum OpenAIModelStreamResponse: Codable {
    case error(OpenAIModelStreamResponseError)
    case response_created(OpenAIModelStreamResponseCreated)
    case response_in_progress(OpenAIModelStreamResponseInProgess)
    case response_completed(OpenAIModelStreamResponseCompleted)
    case response_failed(OpenAIModelStreamResponseFailed)
    case response_incomplete(OpenAIModelStreamResponseInCompleted)
    case response_output_item_added(OpenAIModelStreamResponseOutputItemAdd)
    case response_output_item_done(OpenAIModelStreamResponseOutputItemDone)
    case response_content_part_added(OpenAIModelStreamResponseContentPartAdded)
    case response_content_part_done(OpenAIModelStreamResponseContentPartDone)
    case response_output_text_delta(OpenAIModelStreamResponseOutputTextDelta)
    case response_output_text_annotation_added(OpenAIModelStreamResponseOutputTextAnnotationAdded)
    case response_output_text_done(OpenAIModelStreamResponseOutputTextDone)
    case response_refusal_delta(OpenAIModelStreamResponseRefusalDelta)
    case response_refusal_done(OpenAIModelStreamResponseRefusalDone)
    case response_function_call_arguments_delta(OpenAIModelStreamResponseFunctionCallArgumentsDelta)
    case response_function_call_arguments_done(OpenAIModelStreamResponseFunctionCallArgumentsDone)
    case response_file_search_call_in_progress(OpenAIModelStreamResponseFileSearchCallInProgress)
    case response_file_search_call_searching(OpenAIModelStreamResponseFileSearchCallSearching)
    case response_file_search_call_completed(OpenAIModelStreamResponseFileSearchCallCompleted)
    case response_web_search_call_in_progress(OpenAIModelStreamResponseWebSearchCallInProgress)
    case response_web_search_call_searching(OpenAIModelStreamResponseWebSearchCallSearching)
    case response_web_search_call_completed(OpenAIModelStreamResponseWebSearchCallCompleted)
    case response_reasoning_summary_part_added(OpenAIModelStreamResponseReasoningSummaryPartAdded)
    case response_reasoning_summary_part_done(OpenAIModelStreamResponseReasoningSummaryPartDone)
    case response_reasoning_summary_text_delta(OpenAIModelStreamResponseReasoningSummaryTextDelta)
    case response_reasoning_summary_text_done(OpenAIModelStreamResponseReasoningSummaryTextDone)

    enum CodingKeys: String, CodingKey {
        case type
    }

    public func encode(to encoder: any Encoder) throws {
        var container = encoder.singleValueContainer()
        switch self {
        case .error(let error):
            try container.encode(error)
        case .response_created(let response):
            try container.encode(response)
        case .response_in_progress(let response):
            try container.encode(response)
        case .response_completed(let response):
            try container.encode(response)
        case .response_failed(let response):
            try container.encode(response)
        case .response_incomplete(let response):
            try container.encode(response)
        case .response_output_item_added(let response):
            try container.encode(response)
        case .response_output_item_done(let response):
            try container.encode(response)
        case .response_content_part_added(let response):
            try container.encode(response)
        case .response_content_part_done(let response):
            try container.encode(response)
        case .response_output_text_delta(let response):
            try container.encode(response)
        case .response_output_text_annotation_added(let response):
            try container.encode(response)
        case .response_output_text_done(let response):
            try container.encode(response)
        case .response_refusal_delta(let response):
            try container.encode(response)  
        case .response_refusal_done(let response):
            try container.encode(response)
        case .response_function_call_arguments_delta(let response):
            try container.encode(response)
        case .response_function_call_arguments_done(let response):
            try container.encode(response)
        case .response_file_search_call_in_progress(let response):
            try container.encode(response)
        case .response_file_search_call_searching(let response):
            try container.encode(response)
        case .response_file_search_call_completed(let response):
            try container.encode(response)
        case .response_web_search_call_in_progress(let response):
            try container.encode(response)
        case .response_web_search_call_searching(let response):
            try container.encode(response)
        case .response_web_search_call_completed(let response):
            try container.encode(response)
        case .response_reasoning_summary_part_added(let response):
            try container.encode(response)
        case .response_reasoning_summary_part_done(let response):
            try container.encode(response)
        case .response_reasoning_summary_text_delta(let response):
            try container.encode(response)
        case .response_reasoning_summary_text_done(let response):
            try container.encode(response)
        }
    }

    init(from decoder: any Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        let type = try container.decode(OpenAIModelStreamResponseType.self, forKey: .type)
        switch type {
        case .error:
            self = .error(try OpenAIModelStreamResponseError(from: decoder))
        case .response_created:
            self = .response_created(try OpenAIModelStreamResponseCreated(from: decoder))
        case .response_in_progress:
            self = .response_in_progress(try OpenAIModelStreamResponseInProgess(from: decoder))
        case .response_completed:
            self = .response_completed(try OpenAIModelStreamResponseCompleted(from: decoder))
        case .response_failed:
            self = .response_failed(try OpenAIModelStreamResponseFailed(from: decoder))
        case .response_incomplete:
            self = .response_incomplete(try OpenAIModelStreamResponseInCompleted(from: decoder))
        case .response_output_item_added:
            self = .response_output_item_added(try OpenAIModelStreamResponseOutputItemAdd(from: decoder))
        case .response_output_item_done:
            self = .response_output_item_done(try OpenAIModelStreamResponseOutputItemDone(from: decoder))
        case .response_content_part_added:
            self = .response_content_part_added(try OpenAIModelStreamResponseContentPartAdded(from: decoder))
        case .response_content_part_done:
            self = .response_content_part_done(try OpenAIModelStreamResponseContentPartDone(from: decoder))
        case .response_output_text_delta:
            self = .response_output_text_delta(try OpenAIModelStreamResponseOutputTextDelta(from: decoder))
        case .response_output_text_annotation_added:
            self = .response_output_text_annotation_added(try OpenAIModelStreamResponseOutputTextAnnotationAdded(from: decoder))
        case .response_output_text_done:
            self = .response_output_text_done(try OpenAIModelStreamResponseOutputTextDone(from: decoder))   
        case .response_refusal_delta:
            self = .response_refusal_delta(try OpenAIModelStreamResponseRefusalDelta(from: decoder))
        case .response_refusal_done:
            self = .response_refusal_done(try OpenAIModelStreamResponseRefusalDone(from: decoder))
        case .response_function_call_arguments_delta:
            self = .response_function_call_arguments_delta(try OpenAIModelStreamResponseFunctionCallArgumentsDelta(from: decoder))
        case .response_function_call_arguments_done:
            self = .response_function_call_arguments_done(try OpenAIModelStreamResponseFunctionCallArgumentsDone(from: decoder))
        case .response_file_search_call_in_progress:
            self = .response_file_search_call_in_progress(try OpenAIModelStreamResponseFileSearchCallInProgress(from: decoder))
        case .response_file_search_call_searching:
            self = .response_file_search_call_searching(try OpenAIModelStreamResponseFileSearchCallSearching(from: decoder))
        case .response_file_search_call_completed:
            self = .response_file_search_call_completed(try OpenAIModelStreamResponseFileSearchCallCompleted(from: decoder))
        case .response_web_search_call_in_progress:
            self = .response_web_search_call_in_progress(try OpenAIModelStreamResponseWebSearchCallInProgress(from: decoder))
        case .response_web_search_call_searching:
            self = .response_web_search_call_searching(try OpenAIModelStreamResponseWebSearchCallSearching(from: decoder))
        case .response_web_search_call_completed:
            self = .response_web_search_call_completed(try OpenAIModelStreamResponseWebSearchCallCompleted(from: decoder))
        case .response_reasoning_summary_part_added:
            self = .response_reasoning_summary_part_added(try OpenAIModelStreamResponseReasoningSummaryPartAdded(from: decoder))
        case .response_reasoning_summary_part_done:
            self = .response_reasoning_summary_part_done(try OpenAIModelStreamResponseReasoningSummaryPartDone(from: decoder))
        case .response_reasoning_summary_text_delta:
            self = .response_reasoning_summary_text_delta(try OpenAIModelStreamResponseReasoningSummaryTextDelta(from: decoder))
        case .response_reasoning_summary_text_done:
            self = .response_reasoning_summary_text_done(try OpenAIModelStreamResponseReasoningSummaryTextDone(from: decoder))
        }
    }
}

enum OpenAIModelStreamResponseType: String, Codable {
    case error = "error"
    case response_created = "response.created"
    case response_in_progress = "response.in_progress"
    case response_completed = "response.completed"
    case response_failed = "response.failed"
    case response_incomplete = "response.incomplete"
    case response_output_item_added = "response.output_item.added"
    case response_output_item_done = "response.output_item.done"
    case response_content_part_added = "response.content_part.added"
    case response_content_part_done = "response.content_part.done"
    case response_output_text_delta = "response.output_text.delta"
    case response_output_text_annotation_added = "response.output_text.annotation.added"
    case response_output_text_done = "response.output_text.done"
    case response_refusal_delta = "response.refusal.delta"
    case response_refusal_done = "response.refusal.done"
    case response_function_call_arguments_delta = "response.function_call_arguments.delta"
    case response_function_call_arguments_done = "response.function_call_arguments.done"
    case response_file_search_call_in_progress = "response.file_search_call.in_progress"
    case response_file_search_call_searching = "response.file_search_call.searching"
    case response_file_search_call_completed = "response.file_search_call.completed"
    case response_web_search_call_in_progress = "response.web_search_call.in_progress"
    case response_web_search_call_searching = "response.web_search_call.searching"
    case response_web_search_call_completed = "response.web_search_call.completed"
    case response_reasoning_summary_part_added = "response.reasoning_summary_part.added"
    case response_reasoning_summary_part_done = "response.reasoning_summary_part.done"
    case response_reasoning_summary_text_delta = "response.reasoning_summary_text.delta"
    case response_reasoning_summary_text_done = "response.reasoning_summary_text.done"
}
